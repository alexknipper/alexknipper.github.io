<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
 
 <meta http-equiv="Content-Type" content="text/html;   charset=utf-8"/>
 <meta name="keywords" content=""/>
 <meta name="description" content="<Description>"/>
 <meta name="author" content="Alex Knipper"/>
 <meta name="viewport" content="width=device-width, initial-scale=1">

 <title>Alex Knipper Publications</title>
 
 <link rel="shortcut icon" href="../_static/images/logo/main_icon.ico?"/>

 <link rel="preconnect" href="https://fonts.gstatic.com">
 <link rel="preconnect" href="https://fonts.gstatic.com">
 <link href="https://fonts.googleapis.com/css2?family=Allura&family=Cabin:wght@500&family=Dancing+Script:wght@500&family=Great+Vibes&family=Noto+Sans+JP:wght@400&family=Noto+Serif+JP:wght@500&family=Parisienne&family=Roboto&family=Roboto+Slab&family=Tangerine&display=swap" rel="stylesheet">

 <script src="https://kit.fontawesome.com/b657a3b372.js" crossorigin="anonymous"></script>
 
 <link type="text/css" rel="stylesheet" href="../_static/stylesheets/ak-nav.css"/>
 <link type="text/css" rel="stylesheet" href="../_static/stylesheets/ak-bg-anim.css"/>
 <link type="text/css" rel="stylesheet" href="../_static/stylesheets/style.css"/>
 <link type="text/css" rel="stylesheet" href="../_static/stylesheets/experience.css"/>

 </head>
 <body>
	<div id="ak-bg-anim"></div>
	<header id="ak-nav">
		<a href="../"><img id="ak-nav-icon" src="../_static/images/profile/main.png"/></a>
		<div id="ak-nav-hamburger"></div>
		<nav id="ak-nav-main">
			<div id="ak-nav-links">
				<a href="../portfolio/">Portfolio</a>
				<a href="../academics/">Academics</a>
				<a>Publications</a>
				<a href="../teaching/">Teaching</a>
				<a href="../projects/">Projects</a>
				<!--<a href="./">Side Interests</a>-->
			</div>
			<div id="ak-nav-social">
				<a href="https://www.facebook.com/alex.knipper.37" style="--color:#4064ac"><i class="fab fa-facebook"></i></a>
				<a href="https://www.instagram.com/alex.knipper/" style="--color:#d62977"><i class="fab fa-instagram"></i></a>
				<a href="https://twitter.com/AlexKnipper" style="--color:#1c9cea"><i class="fab fa-twitter"></i></a>
				<a href="https://www.linkedin.com/in/alex-knipper/" style="--color:#0e76a8"><i class="fab fa-linkedin"></i></a>
				<a href="https://github.com/alexknipper" style="--color:#6f42c1"><i class="fab fa-github"></i></a>
				<!--<a href="https://www.youtube.com/channel/UCulNQAsZYn025HDWIvZ1Y2A" style="--color:#e62117"><i class="fab fa-youtube"></i></a>
				<a href="https://alexknipper.bandcamp.com/music" style="--color:#5f95a4"><i class="fab fa-bandcamp"></i></a>-->
			</div>
		</nav>
	</header>
	<div class="content">
		<h1>Publications</h1>
		<p>
			This page lists out all of the publications I have under my name, as well as a couple of featured publications that I find interesting.
		</p>
	</div>
	<div class="content">
		<h2>Featured Publications</h2>
		<details>
			<summary>
				Xue, Linting, Aditya Barua, Noah Constant, Rami Al-Rfou,
				Sharan Narang, Mihir Kale, Adam Roberts and Colin Raffel.
				“ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models.”
				Transactions of the Association for Computational Linguistics 10 (2021): 291-306.
				<a href="https://arxiv.org/pdf/2105.13626.pdf">Paper</a>
			</summary>
			<details>
				<summary>TL;DR</summary>
				<p>
					This paper shows that a standard Transformer architecture can be used
					with minimal modifications to process byte sequences, characterize the
					trade-offs in terms of parameter count, training FLOPs, and inference
					speed, and shows that byte-level models are competitive with their
					token-level counterparts.
				</p>
			</details>
			<details>
				<summary>Abstract</summary>
				<p>
					Most widely used pre-trained language models operate on sequences
					of tokens corresponding to word or subword units. By comparison,
					token-free models that operate directly on raw text
					(bytes or characters) have many benefits:
					They can process text in any language out of the box,
					they are more robust to noise, and they minimize technical debt
					by removing complex and error-prone text preprocessing pipelines.
					Because byte or character sequences are longer than token sequences,
					past work on token-free models has often introduced new model
					architectures designed to amortize the cost of operating directly
					on raw text. In this paper, we show that a standard Transformer
					architecture can be used with minimal modifications to process
					byte sequences. We characterize the trade-offs in terms of
					parameter count, training FLOPs, and inference speed, and show that
					byte-level models are competitive with their token-level counterparts.
					We also demonstrate that byte-level models are significantly more
					robust to noise and perform better on tasks that are sensitive to
					spelling and pronunciation. As part of our contribution, we release
					a new set of pre-trained byte-level Transformer models based on the
					T5 architecture, as well as all code and data used in our experiments.
				</p>
			</details>
		</details>
	</div>
	<div class="content">
		<h2>My Publications</h2>
		<details>
			<summary>
				<strong>[AACL 2022]</strong> - <strong>R. Alexander Knipper,</strong>
				Md. Mahadi Hassan, Mehdi Sadi, Shubhra Kanti Karmaker.
				"Analogy-Guided Evolutionary Pretraining of Binary Word Embeddings".
				In AACL/IJCNLP, 2022.
				<a href="../_static/files/papers/Genetic_Embedding.pdf">Paper</a>
			</summary>
			<details>
				<summary>TL;DR</summary>
				<p>
					This paper proposes a novel genetic algorithm to learn
					the relationships between words from existing word analogy
					data-sets, carefully making sure that the arithmetic properties
					of the relationships are preserved.
				</p>
			</details>
			<details>
				<summary>Abstract</summary>
				<p>
					As we begin to see low-powered computing paradigms (Neuromorphic Computing, Spiking Neural Networks, etc.) becoming more popular, learning binary word embeddings has become increasingly important for supporting NLP applications at the edge. Existing binary word embeddings are mostly derived from pretrained real-valued embeddings through different simple transformations, which often break the semantic consistency and the so-called “arithmetic” properties learned by the original, real-valued embeddings. This paper aims to address this limitation by introducing a new approach to learn binary embeddings from scratch, preserving the semantic relationships between words as well as the arithmetic properties of the embeddings themselves. To achieve this, we propose a novel genetic algorithm to learn the relationships between words from existing word analogy data-sets, carefully making sure that the arithmetic properties of the relationships are preserved. Evaluating our generated 16, 32, and 64-bit binary word embeddings on Mikolov’s word analogy task shows that more than 95% of the time, the best fit for the analogy is ranked in the top 5 most similar words in terms of cosine similarity.
				</p>
			</details>
		</details>
	</div>
 </body>
 <script src="../_static/scripts/themes.js" onload="loadTheme();"></script>
 <script src="../_static/scripts/ak-nav.js" onload="addAKNavbarEvents();"></script>
 <script src="../_static/scripts/ak-bg-anim.js" onload="selectAnimFromDate();"></script>
 </html>